{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How prompts influence LMs?\n",
    "\n",
    "Code for analyzing and comparing LM behavior given prompts of different nature (LAMA, Autoprompt, LPAQA)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Trick to use argparse in the notebook. If argv is given an empty string '' then default arg will be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('code/')\n",
    "\n",
    "from models import build_model_by_name\n",
    "from utils import load_vocab, load_data, batchify, get_relation_meta, output_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle input argument, in this notebook we are only using the default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='opt-350m', help='the huggingface model name')\n",
    "parser.add_argument('--output_dir', type=str, default='output', help='the output directory to store prediction results')\n",
    "parser.add_argument('--common_vocab_filename', type=str, default='common_vocab_cased.txt', help='common vocabulary of models (used to filter triples)')\n",
    "parser.add_argument('--prompt_file', type=str, default='prompts/LAMA_relations.jsonl', help='prompt file containing 41 relations')\n",
    "\n",
    "parser.add_argument('--test_data_dir', type=str, default=\"data/filtered_LAMA\")\n",
    "parser.add_argument('--eval_batch_size', type=int, default=32)\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=6)\n",
    "parser.add_argument('--output_predictions', default=False, help='whether to output top-k predictions')\n",
    "parser.add_argument('--k', type=int, default=5, help='how many predictions will be outputted')\n",
    "parser.add_argument('--device', type=str, default='mps', help='Which computation device: cuda or mps')\n",
    "\n",
    "\n",
    "# Parse arguments\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do various initializations:\n",
    "1. The logger, used to store info related to the experiment.\n",
    "2. Define the init_template function (idk what's the purpose).\n",
    "3. The computation device\n",
    "4. The random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logger\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(args)\n",
    "\n",
    "# A function used to assign template (isn't it?)\n",
    "def init_template(prompt_file, relation):\n",
    "    relation = get_relation_meta(prompt_file, relation)\n",
    "    return relation['template']\n",
    "\n",
    "# Initialize GPUs\n",
    "device=torch.device(args.device)\n",
    "if args.device == 'cuda':\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "elif args.device == 'mps':\n",
    "    n_gpu = 1\n",
    "else:\n",
    "    n_gpu = 0\n",
    "logger.info('# GPUs: %d'%n_gpu)\n",
    "if n_gpu == 0:\n",
    "    logger.warning('No GPU found! exit!')\n",
    "\n",
    "# Random seed\n",
    "random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the LM given the dedicated input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LM\n",
    "model = build_model_by_name(args)\n",
    "logger.info('Model: %s'%args.model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something with the vocabulary. I need to check what is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do something with the vocabulary, idk what\n",
    "if args.common_vocab_filename is not None:\n",
    "    vocab_subset = load_vocab(args.common_vocab_filename)\n",
    "    logger.info('Common vocab: %s, size: %d'%(args.common_vocab_filename, len(vocab_subset)))\n",
    "    filter_indices, index_list = model.init_indices_for_filter_logprobs(vocab_subset)\n",
    "else:\n",
    "    filter_indices = None\n",
    "    index_list = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "The LM iterates on the evaluation data, using a specific prompt types (given as argument, see above)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract neural activations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract neural activation of the ML given as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_activation(model):\n",
    "    activations=None\n",
    "    return activations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, samples_batches, sentences_batches, filter_indices=None, index_list=None, output_topk=None):\n",
    "    \n",
    "    # do some processing on the vocab -> to check\n",
    "    vocab_to_common_vocab = None\n",
    "    if index_list is not None:\n",
    "        vocab_to_common_vocab = {}\n",
    "        for cid, idx in enumerate(index_list):\n",
    "            vocab_to_common_vocab[idx] = cid\n",
    "\n",
    "    cor_all = 0\n",
    "    tot_all = 0\n",
    "    result = {}\n",
    "    list_of_predictions = {}\n",
    "    eval_loss = 0.0\n",
    "    common_eval_loss = 0.0\n",
    "    for i in tqdm(range(len(samples_batches))):\n",
    "        samples_b = samples_batches[i]\n",
    "        sentences_b = sentences_batches[i]\n",
    "\n",
    "        log_probs, cor_b, tot_b, pred_b, topk_preds, loss, common_vocab_loss = model.run_batch(sentences_b, samples_b, training=False, filter_indices=filter_indices, index_list=index_list, vocab_to_common_vocab=vocab_to_common_vocab)\n",
    "        cor_all += cor_b\n",
    "        tot_all += tot_b\n",
    "\n",
    "        for pred, sample, topk, vocab_loss in zip(pred_b, samples_b, topk_preds, common_vocab_loss):\n",
    "            rel = sample['predicate_id']\n",
    "            if rel not in result:\n",
    "                result[rel] = (0, 0, 0, 0.0)\n",
    "                list_of_predictions[rel] = []\n",
    "            cor, tot, _, rel_tot_loss = result[rel]\n",
    "            tot += 1\n",
    "            cor += pred\n",
    "            rel_tot_loss += vocab_loss\n",
    "            result[rel] = (cor, tot, cor / tot if tot > 0 else 0.0, rel_tot_loss)\n",
    "            list_of_predictions[rel].append({\n",
    "                'uuid': sample['uuid'],\n",
    "                'relation': sample['predicate_id'],\n",
    "                'sub_label': sample['sub_label'],\n",
    "                'obj_label': sample['obj_label'],\n",
    "                'masked_sentences': sample['input_sentences'],\n",
    "                'topk': topk,\n",
    "            })\n",
    "        \n",
    "        eval_loss += loss.item() * tot_b\n",
    "    \n",
    "    if output_topk is not None:\n",
    "        logger.info('Output top-k prediction to %s..'%output_topk)\n",
    "        for rel in list_of_predictions:\n",
    "            with open(os.path.join(output_topk, '%s.jsonl'%rel), 'w') as f:\n",
    "                f.write('\\n'.join([json.dumps(x) for x in list_of_predictions[rel]]))\n",
    "\n",
    "    micro, macro = output_result(result, eval_loss)\n",
    "    return micro, result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")\n",
    "for relation in os.listdir(args.test_data_dir):\n",
    "    relation = relation.split(\".\")[0]\n",
    "    logger.info(\"RELATION {}\".format(relation))\n",
    "\n",
    "    output_dir = os.path.join(args.output_dir, os.path.basename(args.prompt_file).split(\".\")[0],args.model_name.replace(\"/\",\"_\"))\n",
    "    os.makedirs(output_dir , exist_ok=True)\n",
    "\n",
    "    template = init_template(args.prompt_file, relation)\n",
    "    logger.info('Template: %s'%template)\n",
    "\n",
    "    test_data = os.path.join(args.test_data_dir, relation + \".jsonl\")\n",
    "    eval_samples = load_data(test_data, template, vocab_subset=vocab_subset, mask_token=model.MASK)\n",
    "    eval_samples_batches, eval_sentences_batches = batchify(eval_samples, args.eval_batch_size * n_gpu)\n",
    "    evaluate(model, eval_samples_batches, eval_sentences_batches, filter_indices, index_list, output_topk=output_dir if args.output_predictions else None)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optiprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "662d219937816f6a927dbe78c9454f7b90e5830c9b780179ed59961fb8578c42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
